\documentclass[a4paper, 12pt]{report}

% Packages
\usepackage[protrusion=false]{microtype}
\usepackage{setspace}

% Language Package
\usepackage[hidelinks]{hyperref}
\usepackage[italian]{babel}
\usepackage[italian]{cleveref}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}
\usepackage[font=footnotesize,labelfont=bf]{caption}

% Environments
\newenvironment{packed_enum}{
\begin{enumerate}
        \setlength{\itemsep}{1pt}
        \setlength{\parskip}{0pt}
        \setlength{\parsep}{0pt}
}{\end{enumerate}}

\newenvironment{packed_itemize}{
\begin{itemize}
        \setlength{\itemsep}{1pt}
        \setlength{\parskip}{0pt}
        \setlength{\parsep}{0pt}
}{\end{itemize}}

% Initialization
\title{Relazione Progetto HPC 2023}
\author{Michele Montesi \\
        Matricola: 0000974934 \\
        E-Mail: michele.montesi3@studio.unibo.it}

\date{\today}

\begin{document}
\maketitle
% \tableofcontents

\chapter*{Introduzione}
\begin{sloppypar}
La presente ricerca presenta lo sviluppo di due versioni parallelizzate del software \texttt{sph.c}, 
implementate utilizzando la libreria \textit{OpenMP} e la libreria \textit{MPI} rispettivamente. 
L'obiettivo della ricerca è quello di valutare i vantaggi prodotti dall'utilizzo della programmazione multiprocessore.
\end{sloppypar}

\bigskip

\begin{sloppypar}
\noindent
Per valutare le prestazioni del software, è stato creato uno script in linguaggio Python che esegue il programma 
con un numero crescente di thread per \textit{OpenMP} e di processi per \textit{MPI}, partendo da 1 e arrivando a 12, e con un numero di particelle che varia da 1400 
a 5700. Lo script registra i dati rilevanti in un foglio di lavoro Excel, dai quali verrà calcolata la media.
Queste informazioni saranno poi utilizzate per valutare le prestazioni.

\bigskip
\noindent
I software sono stati testati sul server \texttt{ISI-Raptor}

\end{sloppypar}

{\let\clearpage\relax\chapter*{Versione OpenMP}}
\section*{Implementazione}
\begin{sloppypar}
  \noindent
  In questa implementazione del software, effettuata con \textit{OpenMP}, sono state parallelizzate le sezioni di codice
  che richiedono maggiore elaborazione utilizzando la direttiva \texttt{\#pragma omp parallel for}.

  \smallskip
  \noindent
  Dove necessario sono state effettuate riduzioni al fine di ottimizzare le prestazioni del software parallelizzato.
\end{sloppypar}

\section*{Prestazioni}
\begin{sloppypar}
  \noindent
  Dalla figura \ref{fig:omp_speedup} si nota come all'aumentare del numero di thread, lo \textit{speedup} aumenti in modo significativo,
  come si può notare dall'aumento della curva. Tuttavia, dopo aver raggiunto 8 thread, si può notare una fase di 
  saturazione, in cui l'aumento del numero di essi non porta più 
  a un miglioramento significativo delle prestazioni. In questo punto, l'utilizzo di thread aggiuntivi potrebbe 
  peggiorare le prestazioni. 

  \begin{figure}[ht]
    \centering
    \includegraphics[width=9cm]{img/omp-speedup.png}
    \caption{Speedup dell'implementazione con OpenMP}
    \label{fig:omp_speedup}
  \end{figure}

  \noindent
  Dal grafico in figura \ref{fig:omp_sse} si può notare come la \textit{Strong Scaling Efficiency} raggiunga
  il suo picco massimo per circa 6 thread, corrispondente a un'efficienza di circa il 90\% rispetto 
  all'efficienza ideale. Dopo il picco, questa inizia a diminuire, il che significa 
  che l'aggiunta di thread aggiuntivi non porta a un miglioramento significativo delle prestazioni, 
  e può addirittura causare un degrado delle prestazioni complessive.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=9cm]{img/omp-sse.png}
    \caption{Strong scaling efficiency dell'implementazione con OpenMP}
    \label{fig:omp_sse}
  \end{figure}

  \newpage
  \noindent
  Nella figura \ref{fig:omp_wse} si può notare come la \textit{Weak Scaling Efficiency} rimanga costante 
  all'aumentare della dimensione del problema. Questo indica che il software è in grado di gestire in 
  modo efficiente problemi di dimensioni diverse, distribuendo il carico di lavoro tra le CPU disponibili 
  senza una diminuzione significativa delle prestazioni. Questa costante è un indicatore
  di una buona scalabilità del software. A 12 thread, l'efficienza debole cala, indicando che l'aggiunta di ulteriori
  non comporta migliori prestazioni.


  \begin{figure}[ht]
    \centering
    \includegraphics[width=9cm]{img/omp-wse.png}
    \caption{Weak scaling efficiency dell'implementazione con OpenMP}
    \label{fig:omp_wse}
  \end{figure}
\end{sloppypar}

{\let\clearpage\relax\chapter*{Versione MPI}}
\section*{Implementazione}
Per l'implementazione del software con \texttt{MPI} è stato creato un \texttt{MPI\_Datatype} contiguo per incapsulare la struttura
dati durante lo scambio di messaggi. Per suddividere le particelle fra tutti i processori viene fatta una \texttt{MPI\_Scatterv} dal processo 0.
Dopo \texttt{compute\_density\_pressure} e \texttt{integrate} viene chiamata una \texttt{MPI\_Allgatherv} per raccogliere i dati elaborati
e ridistribuirli nuovamente a tutti i processori mentre per \texttt{compute\_forces} non viene fatto in quanto questo agisce solamente in locale.
Alla fine, dopo aver calcolato la velocità media delle particelle locali, viene eseguita una \texttt{MPI\_Reduce} con operatore somma.
Questo risultato viene diviso per tutti i processi ottenendo così la velocità media complessiva.

\section*{Prestazioni}
\begin{sloppypar}
  \noindent
  Dalla figura \ref{fig:mpi_speedup} si può notare che le prestazioni del software aumentano in modo 
  quasi lineare all'aumentare del numero di unità di elaborazione utilizzate. Tuttavia, il guadagno di prestazioni in 
  termini di \textit{speedup} diminuisce man mano che se ne aggiungono di nuove, a causa di fattori come l'aumento 
  del tempo di comunicazione tra di esse, la necessità di sincronizzazione tra i processi e la presenza di 
  parti del software che non possono essere parallelizzate. Inoltre, si può notare che lo \textit{speedup} 
  inizia a saturare intorno a 8 core.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=9cm]{img/mpi-speedup.png}
    \caption{Speedup dell'implementazione con MPI}
    \label{fig:mpi_speedup}
  \end{figure}

  \noindent
  Dal grafico in figura \ref{fig:mpi_sse} emerge che la \textit{Strong Scaling Efficiency} raggiunge il suo 
  massimo picco intorno a 6 core, ottenendo un'efficienza di circa il 90\% rispetto all'efficienza ideale. 
  Tuttavia, dopo il picco, l'efficienza inizia a diminuire, indicando che l'aggiunta di ulteriori core non 
  comporta un miglioramento significativo delle prestazioni, rischiando inoltre di peggiorarle in alcuni casi.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=9cm]{img/mpi-sse.png}
    \caption{Strong scaling efficiency dell'implementazione con MPI}
    \label{fig:mpi_sse}
  \end{figure}

  \newpage
  \noindent
  Il grafico della \textit{Weak Scaling Efficiency} (Figura \ref{fig:mpi_wse}) mostra che l'efficienza del software rimane costante 
  al variare della dimensione del problema. Ciò suggerisce che questo è in grado di gestire in modo 
  efficiente problemi di dimensioni diverse, distribuendo il carico di lavoro tra le CPU disponibili senza 
  una diminuzione significativa delle prestazioni. Questo è un indicatore della buona scalabilità del software. 
  Tuttavia, l'aggiunta di ulteriori core oltre i 12 porta a una diminuzione dell'efficienza debole, 
  indicando che l'uso di ulteriori risorse non si traduce in un miglioramento delle prestazioni.

  \begin{figure}[ht]
    \centering
    \includegraphics[width=9cm]{img/mpi-wse.png}
    \caption{Weak scaling efficiency dell'implementazione con MPI}
    \label{fig:mpi_wse}
  \end{figure}

\end{sloppypar}

\chapter*{Conclusioni}
\begin{sloppypar}
  \noindent
  L'implementazione del software con \textit{OpenMP} ha portato a notevoli miglioramenti delle prestazioni, 
  come indicato dal grafico di \textit{speedup} in funzione del numero di \textit{thread}. Si è osservato che l'aumento 
  del numero di questi ha portato a un miglioramento significativo delle prestazioni fino a un certo punto, 
  dopodiché l'effetto positivo si è saturato, indicando che l'utilizzo di ulteriori potrebbe 
  persino peggiorare le prestazioni. La \textit{strong scaling efficiency} è risultata piuttosto elevata 
  per i casi con un numero ridotto di \textit{thread}, 
  ma è diminuita con l'aumento del numero di questi. La \textit{weak scaling efficiency} è stata buona e costante 
  al variare del numero di processori.

  \bigskip
  \noindent
  L'implementazione con \textit{MPI} ha portato a una significativa riduzione dei tempi di esecuzione rispetto 
  alla versione sequenziale del software, come indicato dal grafico di \textit{speedup} in funzione del numero di 
  processori. Si è osservato che la \textit{strong scaling efficiency} è stata molto elevata per tutti i casi analizzati, 
  indicando una buona scalabilità del software. La \textit{weak scaling efficiency} è stata buona e costante 
  al variare del numero di processori.

  \bigskip
  \noindent
  In conclusione, l'implementazione del software con \textit{OpenMP} e \textit{MPI} ha portato a significativi 
  miglioramenti delle prestazioni rispetto alla versione sequenziale. L'approccio OpenMP è 
  particolarmente efficace per problemi di dimensioni limitate e con un numero ridotto di thread, 
  mentre l'approccio \textit{MPI} è più efficiente per problemi di dimensioni maggiori e con 
  un numero elevato di processori.
\end{sloppypar}
  
\end{document}